# -*- coding: utf-8 -*-
"""Automated EDA analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kxRaD_r5hFa_celwkaGMQ-qcY4N_-y0a
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def load_csv_dataset(csv_path):
    # Common missing value representations found in real datasets
    missing_values = [
        "NA", "N/A", "na", "n/a",
        "NULL", "null",
        "None", "none",
        "", " ",
        -999, -99, -1
    ]
    # Load CSV
    df = pd.read_csv(csv_path, na_values=missing_values)
    df.columns = (
        df.columns
        .str.strip()
        .str.lower()
        .str.replace(" ", "_")
    )
    return df

def dataset_overview_and_quality(df, missing_threshold=0.4):
    overview = {}
    overview["num_rows"] = df.shape[0]
    overview["num_columns"] = df.shape[1]
    overview["columns"] = list(df.columns)
    overview["data_types"] = {
        col: str(dtype) for col, dtype in df.dtypes.items()
    }
    missing_counts = df.isna().sum()
    missing_percentages = (missing_counts / len(df)) * 100

    overview["missing_values"] = {
        col: {
            "count": int(missing_counts[col]),
            "percentage": round(missing_percentages[col], 2)
        }
        for col in df.columns
        if missing_counts[col] > 0
    }
    overview["duplicate_rows"] = int(df.duplicated().sum())
    constant_columns = [
        col for col in df.columns
        if df[col].nunique(dropna=True) == 1
    ]
    overview["constant_columns"] = constant_columns
    high_missing_columns = [
        col for col in df.columns
        if missing_percentages[col] >= (missing_threshold * 100)
    ]
    overview["high_missing_columns"] = {
        col: round(missing_percentages[col], 2)
        for col in high_missing_columns
    }
    return overview

def compute_descriptive_statistics(df, text_unique_threshold=0.9):
    stats = {
        "numeric": {},
        "categorical": {}
    }
    num_rows = len(df)
    for col in df.columns:
        col_lower = col.lower()
        if "id" in col_lower:
            continue
        unique_ratio = df[col].nunique(dropna=True) / num_rows
        if pd.api.types.is_numeric_dtype(df[col]):
            # Skip numeric columns that behave like IDs
            if unique_ratio > 0.95:
                continue
            series = df[col].dropna()
            if series.empty:
                continue
            # Basic statistics
            col_min = series.min()
            col_max = series.max()
            col_mean = series.mean()
            col_median = series.median()
            col_mode = series.mode().iloc[0] if not series.mode().empty else None
            col_std = series.std()
            # IQR calculation
            q1 = series.quantile(0.25)
            q3 = series.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            outliers = series[(series < lower_bound) | (series > upper_bound)]
            stats["numeric"][col] = {
                "min": round(col_min, 3),
                "max": round(col_max, 3),
                "mean": round(col_mean, 3),
                "median": round(col_median, 3),
                "mode": round(col_mode, 3) if col_mode is not None else None,
                "std": round(col_std, 3),
                "iqr": round(iqr, 3),
                "outlier_count": int(outliers.count())
            }
        elif pd.api.types.is_object_dtype(df[col]):

            # Skip free-text columns
            if unique_ratio > text_unique_threshold:
                continue

            value_counts = df[col].value_counts(dropna=True)
            percentages = (value_counts / value_counts.sum()) * 100

            stats["categorical"][col] = {
                "counts": value_counts.to_dict(),
                "percentages": percentages.round(2).to_dict()
            }
    return stats

def generate_visualizations(df):
    plots_created = []
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    categorical_cols = df.select_dtypes(include=["object", "category"]).columns.tolist()

    numeric_cols = [
        col for col in numeric_cols
        if df[col].nunique(dropna=True) / len(df) < 0.95 and "id" not in col.lower()
    ]

    if numeric_cols:
        # Choose numeric column with highest variance
        numeric_col = df[numeric_cols].var().idxmax()

        plt.figure()
        df[numeric_col].dropna().hist(bins=30)
        plt.title(f"Histogram of {numeric_col}")
        plt.xlabel(numeric_col)
        plt.ylabel("Frequency")
        plt.show()
        plots_created.append(f"Histogram of {numeric_col}")

        plt.figure()
        plt.boxplot(df[numeric_col].dropna(), vert=False)
        plt.title(f"Boxplot of {numeric_col}")
        plt.xlabel(numeric_col)
        plt.show()
        plots_created.append(f"Boxplot of {numeric_col}")

    if categorical_cols:
        cat_col = categorical_cols[0]
        top_counts = df[cat_col].value_counts(dropna=True).head(10)
        plt.figure()
        top_counts.plot(kind="bar")
        plt.title(f"Top Categories in {cat_col}")
        plt.xlabel(cat_col)
        plt.ylabel("Count")
        plt.tight_layout()
        plt.show()
        plots_created.append(f"Bar chart of {cat_col}")
    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols].corr().abs()
        corr_matrix.values[[range(len(corr_matrix))]*2] = 0
        x_col, y_col = corr_matrix.unstack().idxmax()
        plt.figure()
        plt.scatter(df[x_col], df[y_col], alpha=0.6)
        plt.title(f"Scatter Plot: {x_col} vs {y_col}")
        plt.xlabel(x_col)
        plt.ylabel(y_col)
        plt.show()
        plots_created.append(f"Scatter plot of {x_col} vs {y_col}")
    if len(numeric_cols) >= 2:
        plt.figure(figsize=(8, 6))
        sns.heatmap(df[numeric_cols].corr(), annot=True, cmap="coolwarm")
        plt.title("Correlation Heatmap")
        plt.tight_layout()
        plt.show()
        plots_created.append("Correlation heatmap")
    return plots_created

def generate_insights(overview, stats, max_insights=10):
    insights = []
    limitations = []
    num_rows = overview["num_rows"]
    num_cols = overview["num_columns"]
    insights.append(
        f"The dataset contains {num_rows} rows and {num_cols} columns, "
        "providing a moderate amount of data for exploratory analysis."
    )
    high_missing = overview.get("high_missing_columns", {})
    for col, pct in high_missing.items():
        insights.append(
            f"The column '{col}' has a high missing rate ({pct}%), which may "
            "reduce reliability for analyses involving this variable."
        )
        limitations.append(
            f"High missingness in '{col}' ({pct}%) may introduce bias if the missing data is not random."
        )
    dup_count = overview.get("duplicate_rows", 0)
    if dup_count > 0:
        insights.append(
            f"There are {dup_count} duplicate rows in the dataset, which could "
            "inflate counts or bias summary statistics if not handled."
        )
        limitations.append(
            "Duplicate records may affect aggregate statistics if not removed or accounted for."
        )
    constant_cols = overview.get("constant_columns", [])
    if constant_cols:
        insights.append(
            f"The following columns contain only a single unique value and provide "
            f"no analytical value: {', '.join(constant_cols)}."
        )
    for col, values in stats.get("numeric", {}).items():
        if values["outlier_count"] > 0:
            insights.append(
                f"The numeric column '{col}' contains {values['outlier_count']} outliers "
                "based on the 1.5×IQR rule, suggesting a skewed distribution or extreme values."
            )

        if values["std"] > 0 and values["mean"] != 0:
            cv = values["std"] / abs(values["mean"])
            if cv > 1:
                insights.append(
                    f"The column '{col}' shows high variability relative to its mean, "
                    "indicating substantial spread in the data."
                )
    for col, values in stats.get("categorical", {}).items():
        counts = values["counts"]
        percentages = values["percentages"]

        if counts:
            top_category = max(percentages, key=percentages.get)
            top_pct = percentages[top_category]

            insights.append(
                f"In the categorical column '{col}', the category '{top_category}' "
                f"is the most frequent, representing {top_pct}% of non-missing values."
            )
            if top_pct > 70:
                insights.append(
                    f"The column '{col}' is highly imbalanced, with one category "
                    "dominating the distribution."
                )
    limitations.append(
        "This analysis is purely exploratory and does not account for causal relationships."
    )
    limitations.append(
        "Results may be influenced by data collection methods, sampling strategy, or reporting errors."
    )
    insights = insights[:max_insights]
    return insights, limitations

def run_automated_eda(csv_path):
    print("=" * 60)
    print("AUTOMATED EXPLORATORY DATA ANALYSIS REPORT")
    print("=" * 60)
    # Step 1: Load data
    print("\n[1] Loading dataset...")
    df = load_csv_dataset(csv_path)
    print(f"Dataset loaded successfully with shape: {df.shape}")
    # Step 2: Dataset overview & quality
    print("\n[2] Dataset Overview & Quality Checks")
    overview = dataset_overview_and_quality(df)
    print(f"- Rows: {overview['num_rows']}")
    print(f"- Columns: {overview['num_columns']}")
    print(f"- Duplicate rows: {overview['duplicate_rows']}")
    if overview["constant_columns"]:
        print(f"- Constant columns: {overview['constant_columns']}")
    if overview["high_missing_columns"]:
        print("- High missing columns:")
        for col, pct in overview["high_missing_columns"].items():
            print(f"  • {col}: {pct}% missing")
    # Step 3: Descriptive statistics
    print("\n[3] Descriptive Statistics Summary")
    stats = compute_descriptive_statistics(df)
    print(f"- Numeric columns analyzed: {len(stats['numeric'])}")
    print(f"- Categorical columns analyzed: {len(stats['categorical'])}")
    # Step 4: Visualizations
    print("\n[4] Generating Visualizations...")
    plots = generate_visualizations(df)
    print(f"Generated {len(plots)} plots:")
    for p in plots:
        print(f"  • {p}")
    # Step 5: Insights & limitations
    print("\n[5] Key Insights")
    insights, limitations = generate_insights(overview, stats)
    for i, ins in enumerate(insights, 1):
        print(f"{i}. {ins}")
    print("\n[6] Limitations & Notes")
    for lim in limitations:
        print(f"- {lim}")
    print("\n" + "=" * 60)
    print("END OF AUTOMATED REPORT")
    print("=" * 60)
    # Combined output object
    report = {
        "overview": overview,
        "statistics": stats,
        "plots_generated": plots,
        "insights": insights,
        "limitations": limitations
    }
    return report

report = run_automated_eda("dataset")
